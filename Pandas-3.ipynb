{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> \n",
    "# R406: Using Python for data analysis and modelling\n",
    "\n",
    "<br> <br> \n",
    "\n",
    "## <center> Pandas â€” data cleaning, merging, transformation and reshaping\n",
    "\n",
    "<br>\n",
    "\n",
    "<center> **Andrey Vassilev**\n",
    "\n",
    "<br> \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "\n",
    "1. Merging data\n",
    "2. Cleaning and transforming data\n",
    "3. Reshaping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Merging datasets\n",
    "\n",
    "- In Pandas datasets are merged similarly to database merge operations (\"joins\")\n",
    "- There are different kinds of joins depending on which dataset is the \"leading\" one in the merge operation.\n",
    "- Technically, one can specify different choices of common element(s) that determine the merging operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implicit merges\n",
    "\n",
    "In this case Pandas will automatically try to find common columns to join on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"id\":[112,113,114,116,115],\"x1\":[1,3,2,4,5]})\n",
    "df2 = pd.DataFrame({\"id\":[112,115,114,116,113],\"x2\":[23,13,24,45,44]})\n",
    "display(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Explicit merges on key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"id\":[112,113,114,116,115],\"id1\":[16,14,12,15,13],\"x1\":[1,3,2,4,5]})\n",
    "df2 = pd.DataFrame({\"id\":[112,115,114,116,113],\"id1\":[16,12,14,15,13], \"x2\":[23,13,24,45,44]})\n",
    "display(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,on=\"id1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The keys we are merging on need not have the same names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"id1\":[112,113,114,116,115],\"x1\":[1,3,2,4,5]})\n",
    "df2 = pd.DataFrame({\"id2\":[112,115,114,116,113],\"x2\":[23,13,24,45,44]})\n",
    "display(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,left_on=\"id1\", right_on=\"id2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What happens when the keys match partially?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"id\":[0,113,114,116,115],\"x1\":[1,3,2,4,5]})\n",
    "df2 = pd.DataFrame({\"id\":[112,115,114,116,999],\"x2\":[23,13,24,45,44]})\n",
    "display(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The match is performed only on the common keys. This is called an *inner join*. It is an intersection operation on the keys. Pandas does this by default but we can control it using the `how` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,how=\"inner\") # same as above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The merging operation can be made inclusive by making sure that no key from either `DataFrame` has been left out. This is called an *outer join* and is a union operation on the keys. Missing elements are filled with `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is also possible to have one of the `DataFrame`s as the \"leading\" one and the second one will be merged only where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,how=\"right\")\n",
    "# pd.merge(df2,df1,how=\"left\") # will give the same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also merge on more than one key. Consider these two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"id\":[1,1,2,2,3],\"x1\":[1,3,2,4,5]})\n",
    "df2 = pd.DataFrame({\"id\":[1,1,2,2,3],\"x2\":[23,13,24,45,44]})\n",
    "display(df1,df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is what happens when you merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1,df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now suppose they have an additional key that can serve to uniquely identify rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"id\":[1,1,2,2,3],\"id1\":[1,2,1,2,1],\"x1\":[1,3,2,4,5]})\n",
    "df2 = pd.DataFrame({\"id\":[1,1,2,2,3],\"id1\":[1,2,1,2,1],\"x2\":[23,13,24,45,44]})\n",
    "display(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Merging on index\n",
    "\n",
    "You can also use dataframe indexes as the merge keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = pd.date_range(start=\"2005\",periods=5,freq=\"A\")\n",
    "df1 = pd.DataFrame({\"x1\":[1,3,2,4,5]},index=ind1)\n",
    "df2 = pd.DataFrame({\"x2\":[23,13,24,45,44]},index=ind1)\n",
    "display(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "More complex merges also work as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = pd.date_range(start=\"2005\",periods=5,freq=\"A\")\n",
    "ind2 = pd.date_range(start=\"2004\",periods=5,freq=\"A\")\n",
    "df1 = pd.DataFrame({\"x1\":[1,3,2,4,5]},index=ind1)\n",
    "df2 = pd.DataFrame({\"x2\":[23,13,24,45,44]},index=ind2)\n",
    "display(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,left_index=True,right_index=True,how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that there is also a `join()` method that merges on indexes. Its syntax is a bit more compact then that of `merge()` but we won't deal with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Concatenation\n",
    "\n",
    "Another way of combining datasets is to concatenate them (think stacking them one on top of another)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = pd.date_range(start=\"2000\",periods=5,freq=\"A\")\n",
    "ind2 = pd.date_range(start=\"2004\",periods=5,freq=\"A\")\n",
    "df1 = pd.DataFrame({\"x1\":[1,3,2,4,5]},index=ind1)\n",
    "df2 = pd.DataFrame({\"x1\":[23,13,24,45,44]},index=ind2)\n",
    "display(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Compare with the result of a merge operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,left_index=True,right_index=True,how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformations and data cleaning\n",
    "\n",
    "There are numerous operations that can be classified as \"cleaning\" or \"transforming\" the data. Cleaning is generally any type of operation that removes unnecessary information or handles the case of missing information. Transformations can be even more diverse and obviously can be part of a cleaning operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding and removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"x1\":[1,3,5,7,3],\"x2\":[2,4,6,8,4]})\n",
    "display(df)\n",
    "df.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"x1\":[1,3,5,1,7,3,],\"x2\":[2,4,6,2,8,4]})\n",
    "display(df)\n",
    "df.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transforming data with a function or a map\n",
    "\n",
    "Let's look at the simples case first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['x3'] = 5*df['x1'] - df['x2']**2\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We are obviously not constrained to simple operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def Transf(x):\n",
    "    tmp = x.copy() # What happens if you don't use copy()?\n",
    "    tmp[tmp<0] *= 2\n",
    "    tmp[tmp>0] += 33\n",
    "    return tmp\n",
    "df['x4'] = Transf(df['x3'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or we can use the `map()` method to do the transformation. This allows us to use a function which is not vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df['x5'] = df['x4'].map(lambda x:\"Negative\" if x<0 else \"Positive\" if x>0 else \"Zero\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detecting null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"x5\"]=np.nan\n",
    "df.iloc[1,1]=np.nan\n",
    "df.loc[2,\"x3\"]=None\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dropping NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['x5']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna() # Drops rows by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(axis=1) # Drops columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can consider only a certain column (or columns) when dropping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"x2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `dropna()` method also allows us to:\n",
    "- substitute inplace (as seen previously);\n",
    "- use the `how = 'all'` argument to drop a label only if all entries are missing;\n",
    "- use the `threshold = n` argument to specify that at least `n` values should be missing before dropping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Filling in missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)\n",
    "df.fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "display(df)\n",
    "df.fillna({'x1':1.11,'x2':2.22,'x3':3.33,'x4':4.44})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "display(df)\n",
    "df.fillna(method='backfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "display(df)\n",
    "df.fillna(method='pad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Replacing values\n",
    "\n",
    "We can replace values in general using the `replace()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.fillna({'x1':1.11,'x2':2.22,'x3':3.33,'x4':4.44})\n",
    "display(df1)\n",
    "df1.replace(to_replace = [1.0,2.22,3.33],value=[100,222,333])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also use a dictionary to pass the substitution values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df1)\n",
    "df1.replace({2.22:np.nan,3.33:np.nan})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing dummy variables\n",
    "\n",
    "Sometimes it is useful for modelling purposes to generate a set of dummy variables from a categorical variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],\n",
    "                    'data': range(6)})\n",
    "display(df1)\n",
    "pd.get_dummies(df1['key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can get rid of the `key` variable in our example and replace it with the corresponding dummies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1[['data']],pd.get_dummies(df1['key']),left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discretization and binning\n",
    "\n",
    "We may have to distribute measurements into pre-specified groups, similarly to how one places observations in the different bins of a histogram. This is done with the `cut()` function.\n",
    "\n",
    "As an example, suppose you are given weight measurements on 10 persons and you want to classify them in groups as follows:\n",
    "- up to 50 kg.\n",
    "- between 50 and 60 kg.\n",
    "- between 60 and 90 kg.\n",
    "- ...\n",
    "- above 90 kg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "weights = [49,91,61,88,75,56,45,54,77,71]\n",
    "bins = [0,50,60,70,80,90,np.inf]\n",
    "wbin = pd.cut(weights,bins)\n",
    "wbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the labels\n",
    "wbin.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And these are the groups the observations belong to\n",
    "wbin.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can get a tally of the number of people in each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(wbin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reshaping data\n",
    "\n",
    "This part deals with various ways of representing our dataset by rearranging it from rows to columns and vice versa, making the data \"wide\" or \"long\" etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stacking and unstacking data\n",
    "\n",
    "- The `stack()` method pivots from columns to rows.\n",
    "- The `unstack()` method pivots from rows to columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Stacking makes data \"long\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "display(df)\n",
    "stacked = df.stack()\n",
    "# returns a Series with a hierarchical index\n",
    "display(stacked) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "stacked[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked[4]['x2':'x4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "display(df)\n",
    "stacked = df.stack(dropna=False) # keeps the NaNs\n",
    "display(stacked) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Unstacking works from rows to columns, i.e. makes you data \"wide\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [('California', 2000), ('California', 2010),\n",
    "         ('New York', 2000), ('New York', 2010),\n",
    "         ('Texas', 2000), ('Texas', 2010)]\n",
    "populations = [33871648, 37253956,\n",
    "               18976457, 19378102,\n",
    "               20851820, 25145561]\n",
    "index = pd.MultiIndex.from_tuples(index)\n",
    "pop = pd.Series(populations, index=index)\n",
    "pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pop.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pivoting\n",
    "\n",
    "- The stacking and unstacking operations can be generalized a bit for more convenient use. \n",
    "- This is done through the `pivot()` method, which let us choose what goes on the rows and what â€” on the columns.\n",
    "- It is especially useful for long data in the format usually retrieved from a database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider the following dataset, which contains artificial balance of payments data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'date':[2010,2010,2011,2011,2012,2012],\n",
    "                   'BOPcat':['X','M']*3,\n",
    "                   'valLC':np.array([3000,3000,2900,3100,3050,2950]),\n",
    "                   'valFC':np.array([3000,3000,2900,3100,3050,2950])*2})\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Stacking does not produce very usable results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And neither does unstacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's use the `pivot()` method and instruct it to put the `date` variable on the rows and the `BOPcat` variable on the columns, tabulating the `valLC` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.pivot('date','BOPcat','valLC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can do the same with the `valFC` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.pivot('date','BOPcat','valFC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or swap rows for columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.pivot('BOPcat', 'date', 'valFC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Melting data\n",
    "\n",
    "### The general idea\n",
    "\n",
    "Sometimes your dataset will be organized in such a way that column names contain information that is actually data. Consider the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.DataFrame({'first' : ['John', 'Mary'],\n",
    "                   'last' : ['Doe', 'Bo'],\n",
    "                   'height' : [170, 180],\n",
    "                   'weight' : [60, 80]})\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Here the column names `height` and `weight` themselves contain information on the type of measurement (variable). \n",
    "- This information can be transformed into more compact form if we put it in a separate column and place the corresponding values in another column, like this:  \n",
    "\n",
    "| Variable | Value |\n",
    "| -------- | ----- |\n",
    "| height   | 170   |\n",
    "| height   | 180   |\n",
    "| weight   | 80    |\n",
    "| weight   | 60    |\n",
    "\n",
    "- The above is a basic example of *melting*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- This proposal may not look too different from the original format.\n",
    "- However, imagine that we had observations on more variables like waistline, body fat percentage etc. \n",
    "- These would grow the dataframe horizontally in the original representation while under the proposed transformation having more variables will imply adding row information to a fixed number of columns.\n",
    "- Obviously this process can apply only to some variables (called *measured variables* or *value variables*), as we need to keep certain variables (called *identifier variables*) in order to be able to identify observations uniquely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Pandas implementation of melting\n",
    "\n",
    "The `melt()` function collects the information from the columns (in this case, whether the measurement refers to a person's height or weight) and places it in a new variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt(dt, id_vars=['first', 'last'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `id_vars` list declares certain variables as identifiers and excludes them from the `melt` operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is possible to change the name of the variable to something more expressive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt(dt, id_vars=['first', 'last'], var_name='quantity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To put things in perspective, the `id_vars` are needed in order to avoid losing information. In this case, we use the combination of first and last name to identify which person an observation refers to. Here is the (useless) molten dataframe without this declaration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### More on the rationale behind melting\n",
    "\n",
    "- At this stage one might wonder whether melting is such a good idea: it seems to make a choice in favour of \"long\" rather than \"wide\" data, with the side effect that the readability of the dataset may be worsened in the process of transformation.\n",
    "- However, the primary advantage of melting is that it puts the data in a generic format that is suitable for transformation into different alternative representations, as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Think of it as having the dataset in a database-like format which is convenient for extracting different tables for different purposes.\n",
    "- Actually, the term \"melt\" is used in reference to having molten metal that can be cast into different forms, as desired. Indeed, the statistical computing and graphics environment R uses precisely the term \"cast\" for this reverse operation (recall that in Pandas this is done via the `pivot()` method shown previously)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
