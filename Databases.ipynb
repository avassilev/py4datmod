{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7d27d1e-2e0a-4a51-af5f-fa11e149722e",
   "metadata": {},
   "source": [
    "<center>\n",
    "    \n",
    "# R406: Applied Economic Modelling with Python\n",
    "\n",
    "</center>\n",
    "\n",
    "<br> <br> \n",
    "\n",
    "<center>\n",
    "\n",
    "## An introduction to working with databases in Python\n",
    "\n",
    "</center>\n",
    "\n",
    "<br><br> \n",
    "\n",
    "<center>\n",
    "    \n",
    "## Andrey Vassilev\n",
    "\n",
    "</center>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbb8905-0e61-4838-9115-2392fe884cd1",
   "metadata": {},
   "source": [
    "# Why databases?\n",
    "\n",
    "A **database** is a system designed to store, organize, and efficiently\n",
    "query structured data. Databases play a central role in empirical\n",
    "economics, data science, and machine learning workflows.\n",
    "\n",
    "There are a number of reasons why databases are important:\n",
    "\n",
    "- data may be too large to fit in memory\n",
    "- simple file formats (CSV, Excel, etc.) have important limitations:\n",
    "  - no enforced structure or data types\n",
    "  - no guarantees about consistency\n",
    "  - inefficient for repeated queries on large datasets\n",
    "- data often needs to be queried, filtered, and aggregated repeatedly\n",
    "- data within an organization typically exists in *multiple related datasets (tables)*\n",
    "  (e.g. employees, customers, orders, inventories)\n",
    "- access to data often needs to be controlled and shared across users,\n",
    "  with different permissions and responsibilities\n",
    "- in practice, many organizations store and manage their data in databases,\n",
    "  and analysts are expected to work within these existing workflows\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630d1938-d822-4349-a0cc-2c94e378c85d",
   "metadata": {},
   "source": [
    "# Main types of databases\n",
    "\n",
    "There are many types of databases, designed for different use cases.\n",
    "At a very high level, we can distinguish between:\n",
    "\n",
    "- **Relational databases (SQL)**  \n",
    "  Data is stored in tables with rows and columns, and relationships\n",
    "  between tables are explicit. Examples include:\n",
    "  - SQLite\n",
    "  - PostgreSQL\n",
    "  - MySQL\n",
    "  - DuckDB\n",
    "\n",
    "- **Analytical / column-oriented databases**  \n",
    "  Optimized for fast aggregation and analysis of large datasets. Examples include Amazon Redshift and Monet DB.\n",
    "  DuckDB also belongs to this category.\n",
    "\n",
    "- **NoSQL databases**  \n",
    "  Designed for flexible or unstructured data (e.g. documents,\n",
    "  key–value stores, graphs). Examples include MongoDB and Redis.\n",
    "\n",
    "In what follows, we'll focus on relational databases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e532c17-4968-41e6-bc23-e35871fbdaac",
   "metadata": {},
   "source": [
    "# Python and databases\n",
    "\n",
    "Databases typically expose their functionality through *SQL* (Structured Query Language). \n",
    "We can interact with databases from Python indirectly through specialized libraries that translate to SQL.\n",
    "\n",
    "Common components in the Python database ecosystem include:\n",
    "\n",
    "- **Database drivers**  \n",
    "  Low-level libraries that allow Python to connect to a specific database (e.g. `sqlite3`, `psycopg`, `duckdb`).\n",
    "\n",
    "- **SQLAlchemy**  \n",
    "  A widely used library that provides a common interface for working with many different relational databases. It is often used as a bridge between Python libraries and database engines.\n",
    "\n",
    "- **High-level libraries**  \n",
    "  Libraries such as Pandas build on top of these tools to provide convenient ways to read data from and write data to databases.\n",
    "\n",
    "Here, we will interact with databases through Pandas and DuckDB, without requiring detailed knowledge of SQLAlchemy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1771b5-115a-4168-b9d4-dc48a2b08dca",
   "metadata": {},
   "source": [
    "# Interfacing with a database from Pandas\n",
    "\n",
    "- Pandas provides functions to retrieve data from databases and to write\n",
    "  `DataFrame`s back to databases.\n",
    "- This functionality relies on the `SQLAlchemy` library, which provides\n",
    "  a common interface to many different database systems.\n",
    "- Common functions include:\n",
    "  - `read_sql_table()` to retrieve an entire table from a database,\n",
    "  - `read_sql_query()` to run an SQL query against the database.\n",
    "- A `DataFrame` has a method `to_sql()` to write its contents as a table\n",
    "  in a database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf065cdc-f865-4358-b83d-c60a9fb25512",
   "metadata": {},
   "source": [
    "# Reading tables from a database with Pandas\n",
    "\n",
    "We start by connecting to an existing SQLite database file,\n",
    "`company_data.db`. This database already contains several tables.\n",
    "\n",
    "Pandas provides the function `read_sql_table()` to load an entire table\n",
    "from a database into a `DataFrame`.\n",
    "\n",
    "In this case we happen to know in advance that the database has tables named `firms` and `products`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0528878a-42ed-4e3c-8b3d-4a8c58e6fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\"sqlite:///company_data.db\")\n",
    "\n",
    "firms = pd.read_sql_table(\"firms\", engine)\n",
    "products = pd.read_sql_table(\"products\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c66f681-1e87-4c9a-bf42-565222008016",
   "metadata": {},
   "outputs": [],
   "source": [
    "firms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a385ea-0381-4342-a12b-8f6506fe0633",
   "metadata": {},
   "outputs": [],
   "source": [
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c9e9df-0dff-4ccf-99d3-e127e31f9696",
   "metadata": {},
   "source": [
    "# Writing a Pandas dataframe to a database\n",
    "\n",
    "We now combine the data from the different tables and do additional calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a72148-7785-4844-afad-9ddb42f35d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = firms.merge(products, on=\"firm_id\")\n",
    "merged[\"price_in_EUR\"] = merged[\"price\"]/1.95583\n",
    "merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58bb1dc-3c39-4a9e-907a-c80ca7dfbdc6",
   "metadata": {},
   "source": [
    "It's now time to write the result back to the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1b5dc3-7dcf-49bd-a62e-5ef1f46a6392",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_sql(\"firm_products\", engine, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c99ac23-e564-4dfe-94a2-1a1f1e3306d2",
   "metadata": {},
   "source": [
    "# Running SQL queries from Pandas\n",
    "\n",
    "We now get curious and ask ourselves if our table was successfully written to the database and what other tables are contained there. The following query asks the database which tables it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596bf97c-7068-4fa2-84e7-105a557747ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(\n",
    "    \"SELECT name FROM sqlite_master WHERE type='table'\",\n",
    "    engine\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7903d0-229d-4f6e-abc8-0c53ce5cf2f1",
   "metadata": {},
   "source": [
    "# A quick look at DuckDB\n",
    "\n",
    "DuckDB is a lightweight analytical database designed for fast querying and aggregation. \n",
    "Unlike “server” databases, DuckDB runs *inside* your Python process (similar to SQLite), which makes it very convenient for data work in Jupyter.\n",
    "\n",
    "DuckDB can work seamlessly with:\n",
    "\n",
    "- SQL queries\n",
    "- Pandas DataFrames\n",
    "- files (CSV / Parquet)\n",
    "- Polars DataFrames (preview)\n",
    "\n",
    "**Note:** We will not focus on SQL syntax here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff6cfdc-be40-47c9-b8b5-6e7dd7e2e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# In-memory DuckDB (no files created)\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Quick check\n",
    "con.execute(\"SELECT 1 AS x\").fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a9535-1293-4c40-be31-cca89111b0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative style: connection as a context manager\n",
    "with duckdb.connect() as con2:\n",
    "    print(con2.execute(\"SELECT 2 AS y\").fetchall())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee885c25-715f-4d5f-be9b-e0f291943c4d",
   "metadata": {},
   "source": [
    "# DuckDB + Pandas\n",
    "\n",
    "DuckDB can query Pandas DataFrames directly. This is useful when you\n",
    "already have data in memory but want to use SQL for certain operations\n",
    "(e.g. quick aggregation or combining with data from an external source)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d3fc7-2752-4989-9669-762b652ca1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\"sqlite:///company_data.db\")\n",
    "\n",
    "firms = pd.read_sql_table(\"firms\", engine) # reload just in case\n",
    "products = pd.read_sql_table(\"products\", engine) # reload just in case\n",
    "sales = pd.read_sql_table(\"sales\", engine)\n",
    "\n",
    "display(firms)\n",
    "display(products)\n",
    "display(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c3d15a-4da9-4da2-9d2a-94949e13b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# DuckDB automatically \"sees\" DataFrames referenced in the query\n",
    "revenue_by_firm = con.execute(\"\"\"\n",
    "    SELECT\n",
    "        f.firm_name,\n",
    "        SUM(s.quantity * p.price) AS revenue\n",
    "    FROM sales AS s\n",
    "    JOIN products AS p USING (product_id)\n",
    "    JOIN firms AS f USING (firm_id)\n",
    "    GROUP BY f.firm_name\n",
    "    ORDER BY revenue DESC\n",
    "\"\"\").df()\n",
    "\n",
    "revenue_by_firm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792276be-2537-4798-aff1-df9c2b95963c",
   "metadata": {},
   "source": [
    "# DuckDB + files\n",
    "\n",
    "DuckDB can query data stored in files directly (CSV, Parquet, etc.).\n",
    "This can be useful when:\n",
    "\n",
    "- files are large\n",
    "- you only need a subset of rows/columns\n",
    "- you want to filter/aggregate without loading everything into memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88245165-195a-4558-88e7-63fd174b8fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small CSV file for demonstration\n",
    "sales.to_csv(\"sales.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932afed4-6b54-4811-9a60-018999f50e12",
   "metadata": {},
   "source": [
    "Here is how we can read it and process it in DuckDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9696bdec-e8a8-44ef-83a8-84258b4689c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect()\n",
    "\n",
    "sales_summary = con.execute(\"\"\"\n",
    "    SELECT year, SUM(quantity) AS total_quantity\n",
    "    FROM 'sales.csv'\n",
    "    GROUP BY year\n",
    "    ORDER BY year\n",
    "\"\"\").df()\n",
    "\n",
    "sales_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e52902a-a757-4085-b818-5f6e6e868746",
   "metadata": {},
   "source": [
    "Here is how we can write back to a new file directly from DuckDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed9bce-0ff5-4de3-a2d9-e4ddde334bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"\"\"\n",
    "    COPY (\n",
    "        SELECT year, SUM(quantity) AS total_quantity\n",
    "        FROM 'sales.csv'\n",
    "        GROUP BY year\n",
    "        ORDER BY year\n",
    "    )\n",
    "    TO 'sales_summary.csv' (HEADER, DELIMITER ',')\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5886e27-f06d-497e-be3b-803f5181d91e",
   "metadata": {},
   "source": [
    "# DuckDB + Polars\n",
    "\n",
    "Polars is a fast DataFrame library similar to Pandas.\n",
    "\n",
    "DuckDB mixes seamlessly with Polars and Pandas, which makes it easy to combine Pandas and Polars dataframe operations with SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eacb5dc-81ff-47b0-9a38-fc47550d4d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Convert a Pandas dataframe to a Polars dataframe\n",
    "sales_pl = pl.from_pandas(sales)\n",
    "sales_pl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4779d5-edac-4c34-b4a5-d0002bc5511a",
   "metadata": {},
   "source": [
    "Use DuckDB to connect to our SQLite database, process and output the result as a Polars dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d461ce09-c81a-4d33-a300-3f457b86d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect()\n",
    "\n",
    "# Enable DuckDB's ability to read SQLite databases\n",
    "con.execute(\"INSTALL sqlite_scanner;\")\n",
    "con.execute(\"LOAD sqlite_scanner;\")\n",
    "\n",
    "# Attach the SQLite database file\n",
    "con.execute(\"ATTACH 'company_data.db' AS company_db (TYPE sqlite);\")\n",
    "\n",
    "# Query the SQLite tables using SQL\n",
    "revenue_by_firm_pl = con.execute(\"\"\"\n",
    "    SELECT\n",
    "        f.firm_name,\n",
    "        SUM(s.quantity * p.price) AS revenue\n",
    "    FROM company_db.sales AS s\n",
    "    JOIN company_db.products AS p USING (product_id)\n",
    "    JOIN company_db.firms AS f USING (firm_id)\n",
    "    GROUP BY f.firm_name\n",
    "    ORDER BY revenue DESC\n",
    "\"\"\").pl()\n",
    "\n",
    "revenue_by_firm_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad15cefe-f992-4ac4-a9b7-c433bed104c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars to Pandas\n",
    "revenue_by_firm_pl.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e1f41f-a6a8-43e6-9a3c-1607e4b89c26",
   "metadata": {},
   "source": [
    "Close everything to clean up (not strictly necessary in notebooks but good practice):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f823e64-340a-4ffd-b588-f3da740b9b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.dispose()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea8975a-a7aa-4a56-a8c7-a834a19501aa",
   "metadata": {},
   "source": [
    "# Choosing the right tool\n",
    "\n",
    "The appropriate tool to use depends on the task and the data size.\n",
    "\n",
    "- **Pandas only**\n",
    "  - data fits comfortably in memory\n",
    "  - exploratory analysis, quick transformations\n",
    "  - prototyping models and ideas\n",
    "\n",
    "\n",
    "- **Databases + Pandas**\n",
    "  - data is structured and stored persistently\n",
    "  - multiple related tables\n",
    "  - repeated reads and writes\n",
    "  - integration with existing data workflows\n",
    "\n",
    "\n",
    "- **DuckDB (often with Pandas or Polars)**\n",
    "  - analytical queries and aggregations\n",
    "  - working with large files (CSV, Parquet) without loading everything\n",
    "  - mixing SQL queries with DataFrame-based workflows\n",
    "  - larger-than-memory datasets and lazy evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
