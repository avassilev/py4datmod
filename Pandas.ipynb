{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    \n",
    "# R406: Applied Economic Modelling with Python\n",
    "\n",
    "</center>\n",
    "\n",
    "<br> <br> \n",
    "\n",
    "<center>\n",
    "\n",
    "## Pandas\n",
    "\n",
    "</center>\n",
    "\n",
    "<br><br> \n",
    "\n",
    "<center>\n",
    "<b> Andrey Vassilev </b>\n",
    "</center>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "\n",
    "1. An overview of Pandas\n",
    "2. Main data structures\n",
    "3. Basic operations on Pandas objects\n",
    "4. Importing data\n",
    "  - reading and writing CSV files\n",
    "  - reading and writing Excel files\n",
    "  - Pointers to other functionalities (**Note:** just references here! )\n",
    "     - SQL queries\n",
    "     - Stata, SAS and SPSS (via `savReaderWriter`) files\n",
    "5. Merging data\n",
    "6. Cleaning and transforming data\n",
    "7. Reshaping data\n",
    "8. Aggregation\n",
    "9. An overview of the split-apply-combine concept\n",
    "10. Pivot tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Main facts about Pandas\n",
    "\n",
    "- Pandas is a Python package that offers rich data processing and analysis functionality.\n",
    "- In particular, it can work with series of observations and tabular heterogeneous data (think a dataset consisting of several time series or observations on different subjects).\n",
    "- Pandas allows us to clean, transform, filter, sort etc. a dataset.\n",
    "- Pandas also allows us to split, merge and extract various representations of our data.\n",
    "- Pandas can interact with different data sources.\n",
    "- It has sophisticated date-time functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pandas data structures\n",
    "\n",
    "- The main data structures in Pandas are: \n",
    "    - `Series` \n",
    "    - `DataFrame` \n",
    "- The `Series` is 1D and can be used as a building block of a `DataFrame`\n",
    "- The `DataFrame` is 2D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To start exploring the various Pandas structures we first import the relevant modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # another established convention\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Series\n",
    "\n",
    "A `Series` can be created from a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([1,4,-2,0,np.nan,3])\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A `Series` object has several main characteristics.\n",
    "\n",
    "It has an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of indexing is trivial because it coincides with the familiar indexing for sequences. We can substitute it with more interesting indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.date_range(start=\"2017-01-11\",periods=len(s),freq=\"M\") # Monthly frequency, starting Jan 11, 2017\n",
    "print(dt)\n",
    "s.index=dt\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can inspect the contents of a `Series` by using the `head()` and `tail()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.head(3) # Try changing it to 2 or 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "s.values # You can extract the values as an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[4] = 8 # assignment can be done in a standard way\n",
    "s.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A `Series` can be created from a dictionary. The dictionary keys will be used as index, which will be sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series({\"a\":1,\"b\":3,\"f\":4, \"c\":-2.2})\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create it by simultaneously passing values and index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(np.random.rand(5),index = [\"e\"+str(i) for i in range(1,6)])\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An element of a `Series` can be accessed by its index, \"dictionary-style\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s['e2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or by its position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A `Series` object also supports slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing can be done with respect to the index elements (notice that it is inclusive, unlike position-based slicing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s['e1':'e3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `DataFrame`s\n",
    "\n",
    "The `DataFrame` is the Pandas data structure that holds tabular data. It can be created from a NumPy array and takes an index argument, just like a `Series`. In addition, it takes a `columns` argument specifying column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array([[2,1,3,5],[34,36,29,35]]).T,\n",
    "                  index = pd.date_range(start=\"2005\",periods=4,freq='A'),\n",
    "                  columns = ['A','B'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A `DataFrame` column can be accessed by direct indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['B']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, using a slice will be assumed to refer to the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['A':'B'] \n",
    "# Notice that the error message says that \n",
    "# the string provided is not a date!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Similarly, you need a slice to access rows. The following is an error because Pandas assumes you are trying to provide a column name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2005-12-31']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This already works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2005-12-31':'2006-12-31']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a trivial type of slice if you need to access a single row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2005-12-31':'2005-12-31']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since the previous conventions may be inconvenient in some use cases, Pandas offers a more flexible way to access elements.\n",
    "\n",
    "The `iloc` reference (index location) allows us to specify positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It can be used to access rows and columns simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[2,:] # equivalent to df.iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `loc` functionality allows us to refer by label instead of position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['20071231'] # You can also provide the date string in this format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['20071231':'20071231']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.loc['20061231':'20071231']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.loc['20061231':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,'B']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incidentally, `iloc` and `loc` work also for the indexes of `Series` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is possible to select a custom subset of the data by passing a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[[0,2,3],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpidx = df.index # change the index temporarily\n",
    "                  # to avoid complications with dates\n",
    "df.index = list('abcd')\n",
    "df.loc[['a','c','d'],'B':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore index\n",
    "df.index = tmpidx\n",
    "del tmpidx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ways of creating `DataFrame`s\n",
    "\n",
    "Apart from passing an array, we can also pass a list of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame([[2,1,3,5],[34,36,29,35]],\n",
    "                  index = ['A','B'],\n",
    "                  columns = range(4))\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or we can create the `DataFrame` from a dictionary of `Series` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series(np.random.rand(6),index = range(6,0,-1)) # We can index backward\n",
    "s2 = pd.Series(np.random.rand(6),index = range(6,0,-1)) \n",
    "df1 = pd.DataFrame({'Ser1':s1,'Ser2':s2})\n",
    "df1['Ser1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice what happens when the indexes of the series are different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series(np.random.rand(6),index = range(6,0,-1)) # We can index backward\n",
    "s3 = pd.Series(np.random.rand(6),index = list('abcdef')) \n",
    "df2 = pd.DataFrame({'Ser1':s1,'Ser3':s3})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Indexes\n",
    "\n",
    "The last example hints at some of the properties of indexes. They behave like ordered sets and are designed this way in order to facilitate operations like various joins of datasets.\n",
    "\n",
    "First, an index can be created as an independent object and passed to a `Series` or `DataFrame` constructor later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = pd.Index(list('abcde'))\n",
    "i2 = pd.Index(list('acdghkl'))\n",
    "print(i1) \n",
    "print(i2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can access the elements of an index by position or using a slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2[1:5:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But indexes are immutable. This is a conscious design choice to safeguard the integrity of data transformations and merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This raises an error\n",
    "i2[2] = 'z'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Indexes also support set operations (again useful when combining datasets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 & i2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 | i2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 ^ i2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1.difference(i2) # i1-i2 is deprecated for Index objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# More on selection and assignment\n",
    "\n",
    "A column name of a `DataFrame` can be accessed as an attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.B # equivalent to df['B']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assign using a slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['20051231':'20071231','A'] = [111]*3\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And we can add an entire column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['C'] = np.random.rand(4)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we have been working with numeric values up to here, there nothing to prevent us from having columns of different types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['D'] = ['red', 'blue', 'green', 'yellow']\n",
    "df['E'] = [True, True, False, True]\n",
    "# df.pop('D') \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can delete columns like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['D']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pop('E')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or, if we need to delete many columns, we can just keep what we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = df[['A','B']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Rows in a `DataFrame` can be deleted by means of `drop()`. Note that it returns a copy unless you force in-place changes (either by assignment or by passing `inplace=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.index[0]) # Drop the row that corresponds to the first index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df # still the old one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(df.index[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(df.index[1],inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Replace `df` with a new one to use for the following demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array([[-4.31464978,  4.18579587, -3.95827137,  0.43225809],\n",
    "                           [-1.00034678,  4.32407815,  4.79826565, -4.52343789],\n",
    "                           [ 3.43708467,  1.2913998 ,  4.12525004, -0.55061573],\n",
    "                           [ 3.54330653,  4.45819847,  4.15887073,  4.50748233],\n",
    "                           [ 4.1124862 ,  4.18789329, -1.5093025 ,  3.1387294 ]]), \n",
    "                  index = range(5),columns=list('ABCD'))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Filtering\n",
    "\n",
    "We can filter a dataframe based on a global condition (if it can be evaluated). The entries that fail the condition are filled with `nan`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df>0]\n",
    "# An equivalent way would be df.where(df>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `where()` method allows us to replace the `NaN`s with a specified value or condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(df>0,999)\n",
    "# try also df.where(df>0,-df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also filter a dataframe based on the values of a specific column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['A']<3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ (df['A']>-2) & (df['A']<3.5) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sorting\n",
    "\n",
    "Sometimes we want to rearrange our dataframe based on the values of certain columns. This can be done by using `sort_values()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('A',ascending=False) # sort in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[1,'B'] = df.loc[2,'B']\n",
    "print(df)\n",
    "df.sort_values(['B','C']) # sort by two columns to break ties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# apply ascending vs descending sort to different columns\n",
    "df.sort_values(['B','C'],ascending=[True,False]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting can also be forced to happen in-place using the familiar `inplace` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# If necessary\n",
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reading CSV files\n",
    "\n",
    "The basic function for reading CSV files is `read_csv()`. In its most basic form it takes only a string containing the name of the file to be imported. File will be imported as a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://github.com/fivethirtyeight/data/tree/master/college-majors\n",
    "# for a description of the data\n",
    "gradstudents = pd.read_csv(\"grad-students.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradstudents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some useful parameters (see [docs](http://pandas.pydata.org/pandas-docs/stable/io.html) for a full description):\n",
    " - Pandas will try to infer the separator but if you know your file uses a special delimiter, pass something like `sep = \";\"`\n",
    " - If your data contains a header (=column names) but it is not positioned at row 1 (which is `header = 0` by default), you can skip the first few rows and pass something like `header = 3`. Pass `header = None` if you know your file contains no header.\n",
    " - More generally, you can pass something like `skiprows = 2` (skips the first two rows) or `skiprows = [0,2,3]` (skips specific row numbers) to skip rows at the beginning of a file. The parameter `skipfooter = n` skips the last `n` rows.\n",
    " - The parameter `names = [\"Col1\", \"Col2\"]` will ensure you get specific column names in your `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can read a file that is not in your current working directory. It is done like this:   \n",
    "```mj = pd.read_csv(r\"C:\\Users\\User\\Downloads\\majors-list.csv\")\n",
    "```\n",
    "\n",
    "Or you can even pass a specific URL to retrieve your CSV from the web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "women_stem = pd.read_csv(r\"https://github.com/fivethirtyeight/data/raw/master/college-majors/women-stem.csv\")\n",
    "women_stem.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Writing data to CSV files\n",
    "\n",
    "Data is written to a CSV file using the `to_csv()` method of a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsshort = gradstudents.iloc[0:5,[1,3,5]]\n",
    "print(gsshort)\n",
    "gsshort.to_csv(\"gsshort.csv\", header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `to_csv()` method can also take parameters specifying:\n",
    " - the delimiter: `sep = \";\"`\n",
    " - whether to write the column names as a table header (True by default) but can be `header = False`\n",
    " - whether to write the index (True by default), can be `index = False`\n",
    "\n",
    "It can also take a path different than the current working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reading Excel files\n",
    "\n",
    "Reading an Excel file can be done with the `read_excel()` function. It take a filename or a URL and returns a `DataFrame`. It can take other arguments, such a specific sheet name to read the data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to changes in the BoE website this cell no longer works. It needs to be changed or deleted.\n",
    "# ratesraw = pd.read_excel(r\"http://www.bankofengland.co.uk/statistics/Documents/dl/251115fsg.xls\", sheet_name= \"Data\")\n",
    "# ratesraw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Other arguments can include skipping a specific number of rows, including a custom set of column names etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = pd.read_excel(r\"251115fsg.xls\", \n",
    "                      sheet_name = \"Data\", header = None, skiprows = 4, \n",
    "                      names = [\"date\",\"r\"])\n",
    "rates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also take the index from a specific column etc. In general, the approach and syntax are similar to those for CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates1 = pd.read_excel(r\"251115fsg.xls\", \n",
    "                      sheet_name = \"Data\", header = None, skiprows = 4, \n",
    "                      names = [\"r\"], index_col=0)\n",
    "rates1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Writing Excel files\n",
    "\n",
    "**Big time warning: If you have an Excel file with the same name, the method shown below will essentially delete it and recreate it, including only the data from your dataframe. It will NOT update only specific sheets or ranges in an existing spreadsheet. If you need more advanced functionality, such as writing data to a specific range in a specific sheet, look elsewhere (e.g. the `openpyxl` library). **\n",
    "\n",
    "The dataframe method for writing to Excel is called `to_excel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates1[\"r\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates1.to_excel(\"rates1.xlsx\",sheet_name=\"rates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Again, you can pass a number of parameters. For instance, as shown below, you can choose not to include the `DataFrame` index and columns. You can also specify a specific starting place in the sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates1.to_excel(\"rates1.xlsx\", sheet_name=\"rates\", header = False, \n",
    "                index = False, startcol=3, startrow=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Getting data directly from statistical sources\n",
    "\n",
    "- Pandas has an associated library called `pandas_datareader` which facilitates access to information from several popular data sources.\n",
    "- Examples include (see [here](https://pandas-datareader.readthedocs.io/en/latest/remote_data.html) for the full list):\n",
    "    - Eurostat      \n",
    "    - World Bank\n",
    "    - OECD\n",
    "    - Yahoo! Finance\n",
    "    - Google Finance\n",
    "    - St.Louis FED (FRED)\n",
    "- We shall look at the first three to get an idea how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Getting a Eurostat dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas_datareader.data as web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table tps00001 reports the population of a country \n",
    "# on 1 January of the respective year.\n",
    "df = web.DataReader(\"tps00001\", 'eurostat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An aside on working with `MultiIndex` object\n",
    "\n",
    "- The previous example shows that the columns are represented by a complex object, a `MultiIndex`. \n",
    "- This is essentially a nested structure of column names with headings, subheadings etc.\n",
    "- Here are a few common operations on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Selection\n",
    "df[\"Population on 1 January - total\"][\"Albania\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# One more level, produces a Series\n",
    "df[\"Population on 1 January - total\"][\"Albania\"][\"Annual\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Selecting several at once\n",
    "df[\"Population on 1 January - total\"][[\"Albania\",\"Azerbaijan\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# You can reassign the columns to simplify the structure\n",
    "df1 = df[\"Population on 1 January - total\"][[\"Albania\",\"Azerbaijan\"]]\n",
    "df1.columns = [\"Albania\",\"Azerbaijan\"]\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Interfacing with a database\n",
    "\n",
    "**Note: No code examples here. Given for general info.**\n",
    "\n",
    "- Pandas has functions to retrieve information from databases and write back `DataFrame`s to databases.\n",
    "- This relies on using the powerful `SQLAlchemy` library\n",
    "- There are functions such as: \n",
    "   - `read_sql_table()` to retrieve a table from a database\n",
    "   - `read_sql_query()` to run a query against the database\n",
    "- A dataframe has a method `to_sql()` to write it as a table in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Working with Stata, SAS and SPSS files\n",
    "\n",
    "**Note: No code examples here. Given for general info.**\n",
    "\n",
    "- These file formats are relatively popular and you may have to read in and process data packaged in one of them.\n",
    "- The respective native Pandas methods are:\n",
    "  - `read_stata()`\n",
    "  - `read_sas()`\n",
    "- Pandas does not work natively with SPSS files. The `savReaderWriter` module provides IO functions to work with the `sav` format.\n",
    "- As a general rule, if you need to export data from Python to another program, the safest choice is probably to export in plain-text format and then read it into the other application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data cleaning, merging, transformation and reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# If necessary\n",
    "%reset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Merging datasets\n",
    "\n",
    "- In Pandas datasets are merged similarly to database merge operations (\"joins\")\n",
    "- There are different kinds of joins depending on which dataset is the \"leading\" one in the merge operation.\n",
    "- Technically, one can specify different choices of common element(s) that determine the merging operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implicit merges\n",
    "\n",
    "In this case Pandas will automatically try to find common columns to join on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"id\":[112,113,114,116,115],\"x1\":[1,3,2,4,5]})\n",
    "df2 = pd.DataFrame({\"id\":[112,115,114,116,113],\"x2\":[23,13,24,45,44]})\n",
    "display(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Explicit merges on key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"id\":[112,113,114,116,115],\"id1\":[16,14,12,15,13],\"x1\":[1,3,2,4,5]})\n",
    "df2 = pd.DataFrame({\"id\":[112,115,114,116,113],\"id1\":[16,12,14,15,13], \"x2\":[23,13,24,45,44]})\n",
    "display(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,on=\"id1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The keys we are merging on need not have the same names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"id1\":[112,113,114,116,115],\"x1\":[1,3,2,4,5]})\n",
    "df2 = pd.DataFrame({\"id2\":[112,115,114,116,113],\"x2\":[23,13,24,45,44]})\n",
    "display(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,left_on=\"id1\", right_on=\"id2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What happens when the keys match partially?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"id\":[0,113,114,116,115],\"x1\":[1,3,2,4,5]})\n",
    "df2 = pd.DataFrame({\"id\":[112,115,114,116,999],\"x2\":[23,13,24,45,44]})\n",
    "display(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The match is performed only on the common keys. This is called an *inner join*. It is an intersection operation on the keys. Pandas does this by default but we can control it using the `how` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,how=\"inner\") # same as above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The merging operation can be made inclusive by making sure that no key from either `DataFrame` has been left out. This is called an *outer join* and is a union operation on the keys. Missing elements are filled with `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is also possible to have one of the `DataFrame`s as the \"leading\" one and the second one will be merged only where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,how=\"right\")\n",
    "# pd.merge(df2,df1,how=\"left\") # will give the same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also merge on more than one key. Consider these two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"id\":[1,1,2,2,3],\"x1\":[1,3,2,4,5]})\n",
    "df2 = pd.DataFrame({\"id\":[1,1,2,2,3],\"x2\":[23,13,24,45,44]})\n",
    "display(df1,df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is what happens when you merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1,df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now suppose they have an additional key that can serve to uniquely identify rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"id\":[1,1,2,2,3],\"id1\":[1,2,1,2,1],\"x1\":[1,3,2,4,5]})\n",
    "df2 = pd.DataFrame({\"id\":[1,1,2,2,3],\"id1\":[1,2,1,2,1],\"x2\":[23,13,24,45,44]})\n",
    "display(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Merging on index\n",
    "\n",
    "You can also use dataframe indexes as the merge keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = pd.date_range(start=\"2005\",periods=5,freq=\"A\")\n",
    "df1 = pd.DataFrame({\"x1\":[1,3,2,4,5]},index=ind1)\n",
    "df2 = pd.DataFrame({\"x2\":[23,13,24,45,44]},index=ind1)\n",
    "display(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "More complex merges also work as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = pd.date_range(start=\"2005\",periods=5,freq=\"A\")\n",
    "ind2 = pd.date_range(start=\"2004\",periods=5,freq=\"A\")\n",
    "df1 = pd.DataFrame({\"x1\":[1,3,2,4,5]},index=ind1)\n",
    "df2 = pd.DataFrame({\"x2\":[23,13,24,45,44]},index=ind2)\n",
    "display(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,left_index=True,right_index=True,how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that there is also a `join()` method that merges on indexes. Its syntax is a bit more compact then that of `merge()` but we won't deal with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Concatenation\n",
    "\n",
    "Another way of combining datasets is to concatenate them (think stacking them one on top of another)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = pd.date_range(start=\"2000\",periods=5,freq=\"A\")\n",
    "ind2 = pd.date_range(start=\"2004\",periods=5,freq=\"A\")\n",
    "df1 = pd.DataFrame({\"x1\":[1,3,2,4,5]},index=ind1)\n",
    "df2 = pd.DataFrame({\"x1\":[23,13,24,45,44]},index=ind2)\n",
    "display(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Compare with the result of a merge operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1,df2,left_index=True,right_index=True,how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformations and data cleaning\n",
    "\n",
    "There are numerous operations that can be classified as \"cleaning\" or \"transforming\" the data. Cleaning is generally any type of operation that removes unnecessary information or handles the case of missing information. Transformations can be even more diverse and obviously can be part of a cleaning operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding and removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"x1\":[1,3,5,7,3],\"x2\":[2,4,6,8,4]})\n",
    "display(df)\n",
    "df.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"x1\":[1,3,5,1,7,3,],\"x2\":[2,4,6,2,8,4]})\n",
    "display(df)\n",
    "df.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transforming data with a function or a map\n",
    "\n",
    "Let's look at the simples case first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['x3'] = 5*df['x1'] - df['x2']**2\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We are obviously not constrained to simple operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def Transf(x):\n",
    "    tmp = x.copy() # What happens if you don't use copy()?\n",
    "    tmp[tmp<0] *= 2\n",
    "    tmp[tmp>0] += 33\n",
    "    return tmp\n",
    "df['x4'] = Transf(df['x3'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or we can use the `map()` method to do the transformation. This allows us to use a function which is not vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df['x5'] = df['x4'].map(lambda x:\"Negative\" if x<0 else \"Positive\" if x>0 else \"Zero\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detecting null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"x5\"]=np.nan\n",
    "df.iloc[1,1]=np.nan\n",
    "df.loc[2,\"x3\"]=None\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dropping NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['x5']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna() # Drops rows by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(axis=1) # Drops columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can consider only a certain column (or columns) when dropping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"x2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `dropna()` method also allows us to:\n",
    "- substitute inplace (as seen previously);\n",
    "- use the `how = 'all'` argument to drop a label only if all entries are missing;\n",
    "- use the `threshold = n` argument to specify that at least `n` values should be missing before dropping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Filling in missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)\n",
    "df.fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "display(df)\n",
    "df.fillna({'x1':1.11,'x2':2.22,'x3':3.33,'x4':4.44})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "display(df)\n",
    "df.fillna(method='backfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "display(df)\n",
    "df.fillna(method='pad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Replacing values\n",
    "\n",
    "We can replace values in general using the `replace()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.fillna({'x1':1.11,'x2':2.22,'x3':3.33,'x4':4.44})\n",
    "display(df1)\n",
    "df1.replace(to_replace = [1.0,2.22,3.33],value=[100,222,333])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also use a dictionary to pass the substitution values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df1)\n",
    "df1.replace({2.22:np.nan,3.33:np.nan})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing dummy variables\n",
    "\n",
    "Sometimes it is useful for modelling purposes to generate a set of dummy variables from a categorical variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],\n",
    "                    'data': range(6)})\n",
    "display(df1)\n",
    "pd.get_dummies(df1['key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can get rid of the `key` variable in our example and replace it with the corresponding dummies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1[['data']],pd.get_dummies(df1['key']),left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discretization and binning\n",
    "\n",
    "We may have to distribute measurements into pre-specified groups, similarly to how one places observations in the different bins of a histogram. This is done with the `cut()` function.\n",
    "\n",
    "As an example, suppose you are given weight measurements on 10 persons and you want to classify them in groups as follows:\n",
    "- up to 50 kg.\n",
    "- between 50 and 60 kg.\n",
    "- between 60 and 90 kg.\n",
    "- ...\n",
    "- above 90 kg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "weights = [49,91,61,88,75,56,45,54,77,71]\n",
    "bins = [0,50,60,70,80,90,np.inf]\n",
    "wbin = pd.cut(weights,bins)\n",
    "wbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the labels\n",
    "wbin.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And these are the groups the observations belong to\n",
    "wbin.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can get a tally of the number of people in each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(wbin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reshaping data\n",
    "\n",
    "This part deals with various ways of representing our dataset by rearranging it from rows to columns and vice versa, making the data \"wide\" or \"long\" etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stacking and unstacking data\n",
    "\n",
    "- The `stack()` method pivots from columns to rows.\n",
    "- The `unstack()` method pivots from rows to columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Stacking makes data \"long\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "display(df)\n",
    "stacked = df.stack()\n",
    "# returns a Series with a hierarchical index\n",
    "display(stacked) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "stacked[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked[4]['x2':'x4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "display(df)\n",
    "stacked = df.stack(dropna=False) # keeps the NaNs\n",
    "display(stacked) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Unstacking works from rows to columns, i.e. makes you data \"wide\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [('California', 2000), ('California', 2010),\n",
    "         ('New York', 2000), ('New York', 2010),\n",
    "         ('Texas', 2000), ('Texas', 2010)]\n",
    "populations = [33871648, 37253956,\n",
    "               18976457, 19378102,\n",
    "               20851820, 25145561]\n",
    "index = pd.MultiIndex.from_tuples(index)\n",
    "pop = pd.Series(populations, index=index)\n",
    "pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pop.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pivoting\n",
    "\n",
    "- The stacking and unstacking operations can be generalized a bit for more convenient use. \n",
    "- This is done through the `pivot()` method, which let us choose what goes on the rows and what  on the columns.\n",
    "- It is especially useful for long data in the format usually retrieved from a database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider the following dataset, which contains artificial balance of payments data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'date':[2010,2010,2011,2011,2012,2012],\n",
    "                   'BOPcat':['X','M']*3,\n",
    "                   'valLC':np.array([3000,3000,2900,3100,3050,2950]),\n",
    "                   'valFC':np.array([3000,3000,2900,3100,3050,2950])*2})\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Stacking does not produce very usable results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And neither does unstacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's use the `pivot()` method and instruct it to put the `date` variable on the rows and the `BOPcat` variable on the columns, tabulating the `valLC` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.pivot('date','BOPcat','valLC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can do the same with the `valFC` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.pivot('date','BOPcat','valFC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or swap rows for columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.pivot('BOPcat', 'date', 'valFC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Melting data\n",
    "\n",
    "### The general idea\n",
    "\n",
    "Sometimes your dataset will be organized in such a way that column names contain information that is actually data. Consider the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.DataFrame({'first' : ['John', 'Mary'],\n",
    "                   'last' : ['Doe', 'Bo'],\n",
    "                   'height' : [170, 180],\n",
    "                   'weight' : [60, 80]})\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Here the column names `height` and `weight` themselves contain information on the type of measurement (variable). \n",
    "- This information can be transformed into more compact form if we put it in a separate column and place the corresponding values in another column, like this:  \n",
    "\n",
    "| Variable | Value |\n",
    "| -------- | ----- |\n",
    "| height   | 170   |\n",
    "| height   | 180   |\n",
    "| weight   | 80    |\n",
    "| weight   | 60    |\n",
    "\n",
    "- The above is a basic example of *melting*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- This proposal may not look too different from the original format.\n",
    "- However, imagine that we had observations on more variables like waistline, body fat percentage etc. \n",
    "- These would grow the dataframe horizontally in the original representation while under the proposed transformation having more variables will imply adding row information to a fixed number of columns.\n",
    "- Obviously this process can apply only to some variables (called *measured variables* or *value variables*), as we need to keep certain variables (called *identifier variables*) in order to be able to identify observations uniquely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Pandas implementation of melting\n",
    "\n",
    "The `melt()` function collects the information from the columns (in this case, whether the measurement refers to a person's height or weight) and places it in a new variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt(dt, id_vars=['first', 'last'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `id_vars` list declares certain variables as identifiers and excludes them from the `melt` operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is possible to change the name of the variable to something more expressive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt(dt, id_vars=['first', 'last'], var_name='quantity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To put things in perspective, the `id_vars` are needed in order to avoid losing information. In this case, we use the combination of first and last name to identify which person an observation refers to. Here is the (useless) molten dataframe without this declaration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### More on the rationale behind melting\n",
    "\n",
    "- At this stage one might wonder whether melting is such a good idea: it seems to make a choice in favour of \"long\" rather than \"wide\" data, with the side effect that the readability of the dataset may be worsened in the process of transformation.\n",
    "- However, the primary advantage of melting is that it puts the data in a generic format that is suitable for transformation into different alternative representations, as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Think of it as having the dataset in a database-like format which is convenient for extracting different tables for different purposes.\n",
    "- Actually, the term \"melt\" is used in reference to having molten metal that can be cast into different forms, as desired. Indeed, the statistical computing and graphics environment R uses precisely the term \"cast\" for this reverse operation (recall that in Pandas this is done via the `pivot()` method shown previously)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data aggregation, the split-apply-combine paradigm and pivot tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# If necessary\n",
    "%reset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aggregation operations for `Series`\n",
    "\n",
    "These practically mirror the respective operations for arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(5)\n",
    "s = pd.Series(rng.rand(5))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many other operations are available. Here are a few examples to give you ideas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.cumprod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aggregation operations for `DataFrames`\n",
    "\n",
    "This is the same in spirit to the operations for `Series`. The novelty here is the option to perform an operation rowwise or columnwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "wght = pd.DataFrame({'Bob':[90,91,89,88,86],\n",
    "                     'Jane':[68,62,61,59,59], \n",
    "                     'Joe':[75,76,77,79,80]},\n",
    "                 index=pd.date_range(start=\"20160601\",\n",
    "                                     periods=5,freq='M'))\n",
    "wght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wght.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wght.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "wght.mean(axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "wage = pd.DataFrame({'Bill':[1000,1100,1050,1000,1200],'Jill':[2000,2000,2000,3000,2000], 'Jane':[500,550,550,600,500]},\n",
    "                 index=pd.date_range(start=\"20160101\",periods=5,freq='M'))\n",
    "wage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wage.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wage.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "otherincome = pd.DataFrame({'Bill':[2000,2100,2050,2000,2200],'Jill':[3000,3000,3000,4000,3000], 'Jane':[1500,1550,1550,1600,1500]},\n",
    "                 index=pd.date_range(start=\"20160101\",periods=5,freq='M'))\n",
    "display(otherincome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In many respects a `DataFrame` behaves just like a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wage + otherincome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above\n",
    "wage.add(otherincome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "wage*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Split-apply-combine operations\n",
    "\n",
    "- A common need that arises in data analysis is to divide a dataset into several subsets according to some criterion, process and analyse these subsets separately and put the results back together.\n",
    "- This workflow is known as **split-apply-combine**.\n",
    "- Pandas supports this approach via the `groupby` operation.\n",
    "- The next slide contains a nice illustration of the main idea (courtesy of Jake VanderPlas's *Python Data Science Handbook*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Split-apply-combine illustrated](http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/03.08-split-apply-combine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The diamonds dataset\n",
    "\n",
    "Source: R's `ggplot2` package\n",
    "\n",
    "Description taken from http://docs.ggplot2.org/current/diamonds.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Variables:**  \n",
    "- price. price in US dollars (\\\\$326-\\\\$18,823)\n",
    "- carat. weight of the diamond (0.2-5.01)\n",
    "- cut. quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n",
    "- colour. diamond colour, from J (worst) to D (best)\n",
    "- clarity. a measurement of how clear the diamond is (I1 (worst), SI1, SI2, VS1, VS2, VVS1, VVS2, IF (best))\n",
    "- x. length in mm (0-10.74)\n",
    "- y. width in mm (0-58.9)\n",
    "- z. depth in mm (0-31.8)\n",
    "- depth. total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43-79)\n",
    "- table. width of top of diamond relative to widest point (43-95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dia = pd.read_csv(\"diamonds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dia.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `unique()` method of a series allows us to get the distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dia['color'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dia.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dia.groupby('cut').describe() # This is a GroupBy object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can refer to a column of a `GroupBy` object and invoke a method on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dia.groupby('cut')['carat'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Or we can iterate over the group members. This produces tuples of group names and dataframes corresponding to the respective group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in dia.groupby('cut'):\n",
    "    print(group[0])\n",
    "    print(type(group[1]))\n",
    "\n",
    "# Or, almost equivalently\n",
    "# for gr, fr in dia.groupbys('cut'):\n",
    "#     print(gr,type(fr),sep=\": \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can get a particular group with `get_group()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dia.groupby('cut').get_group('Good').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Operations on groups\n",
    "\n",
    "### Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dia.groupby('cut').agg([sum,min,max])\n",
    "# Note the results for string variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can specify the operations to be column-specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dia.groupby('cut').agg({'carat':np.mean, 'price':max})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dia.groupby('cut').agg({'carat':np.mean, 'price':[min,max]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Filtering\n",
    "\n",
    "We may want to keep only groups that satisfy certain conditions. Let's say we want to keep only those groups which have more than 30 diamonds with a price above \\$18500. We can do it with the `filter()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SelectManyExpensive(x):\n",
    "    return sum(x['price'] > 18500) > 30\n",
    "dia.groupby('cut').filter(SelectManyExpensive).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dia.groupby('cut').filter(SelectManyExpensive).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dia.groupby('cut').filter(SelectManyExpensive)['cut'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transforming\n",
    "\n",
    "The `transform()` method allows us to apply a transformation to each group. Here is a group-specific standardization transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dia.loc[:,['cut','table','price']].groupby(\n",
    "    'cut').transform(lambda x: (x - x.mean()) / x.std()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Apply operations\n",
    "\n",
    "The `apply()` method is similar to the transform method  with the difference that the function passed to the method takes a dataframe to perform some calculation and returns a Pandas object or a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "def ReturnSlope(x):\n",
    "    return linregress(x['price'],x['carat']).slope\n",
    "dia.groupby('cut').apply(ReturnSlope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pivot tables\n",
    "\n",
    "We already know the reshaping operation `pivot`. Pivot tables carry this idea further by providing data aggregation functionality. The basic syntax is `pivot_table(values, index, columns)` with aggregation performed using the `mean` function by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dia.pivot_table(values='price',index='cut',columns='color')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dia.pivot_table(values='price',index='cut',columns='color',aggfunc=sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can pass several aggregating functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dia.pivot_table(values='price',index='cut',columns='color',aggfunc=[min,max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also work with hierarchical indexes or columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dia.pivot_table(values='price',index=['cut','clarity'],columns='color').head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dia.pivot_table(values='price',index='color',columns=['cut','clarity']).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can add margins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dia.pivot_table(values='price',index='color',columns='clarity',margins=True,aggfunc=sum).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And choose a fill value for NAs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dia.pivot_table(values='price',index='color',columns=['cut','clarity'],fill_value=-1).head(15)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
